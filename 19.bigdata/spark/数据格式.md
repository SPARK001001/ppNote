**Spark 数据格式：** 知道 Spark 支持的数据格式，如 Parquet、Avro、JSON、CSV 等，以及如何在 Spark 中读取和写入这些格式的数据。

- Spark 支持多种数据格式，可以通过 DataFrame API 或 Spark SQL 读取和写入这些格式的数据。以下是一些常见的 Spark 数据格式以及如何在 Spark 中操作它们：

  1. **Parquet：** Parquet 是一种列式存储格式，适用于大规模数据存储和分析。在 Spark 中，您可以使用 `spark.read.parquet()` 来读取 Parquet 格式的数据，使用 `DataFrame.write.parquet()` 来写入 Parquet 格式的数据。

  2. **Avro：** Avro 是一种二进制格式，支持架构演化。您可以使用 `spark.read.format("avro")` 来读取 Avro 格式的数据，使用 `DataFrame.write.format("avro")` 来写入 Avro 格式的数据。

  3. **JSON：** JSON 是一种常见的文本格式，用于数据交换。您可以使用 `spark.read.json()` 来读取 JSON 格式的数据，使用 `DataFrame.write.json()` 来写入 JSON 格式的数据。

  4. **CSV：** CSV 是逗号分隔的文本格式，也是常见的数据交换格式。您可以使用 `spark.read.csv()` 来读取 CSV 格式的数据，使用 `DataFrame.write.csv()` 来写入 CSV 格式的数据。

  5. **ORC：** ORC（Optimized Row Columnar）是一种列式存储格式，适用于大规模数据分析。您可以使用 `spark.read.format("orc")` 来读取 ORC 格式的数据，使用 `DataFrame.write.format("orc")` 来写入 ORC 格式的数据。

  6. **Text：** 您可以使用 `spark.read.text()` 来读取文本文件的数据。

  7. **Hive 表：** 如果使用了 Hive 作为元数据存储，您可以通过 `spark.table("tableName")` 来读取 Hive 表中的数据。

  8. **自定义数据源：** Spark 还支持通过自定义数据源来读取和写入其他数据格式。

  通过选择适当的数据格式，您可以在 Spark 中高效地读取和处理不同类型的数据，从而进行数据分析和处理任务。